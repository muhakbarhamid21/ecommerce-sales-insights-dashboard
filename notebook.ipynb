{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wADwK78DCz"
      },
      "source": [
        "# Data Analysis Project: E-Commerce Public Dataset\n",
        "- **Name:** Muhammad Akbar Hamid\n",
        "- **Email:** muhakbarhamid21@gmail.com\n",
        "- **ID Dicoding:** muhakbarhamid21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE0raob58DC0"
      },
      "source": [
        "## Defining Business Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmQeQ5YF8DC0",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "source": [
        "1. **Which product categories have the highest and lowest sales volumes?**  \n",
        "   This question aims to identify the top-performing product categories, as well as those that are underperforming in terms of sales volume, to guide inventory and marketing strategies.\n",
        "\n",
        "2. **What are the monthly sales trends, and how have they evolved over time?**  \n",
        "   This question focuses on analyzing the overall performance of monthly sales, allowing for the identification of seasonal patterns or other fluctuations over time.\n",
        "\n",
        "3. **RFM Analysis**:\n",
        "   - **When did customers last make a transaction, and what does this tell us about customer recency?**  \n",
        "     This will help determine how recently customers have engaged with the business and identify those who may require re-engagement.\n",
        "\n",
        "   - **How frequently are customers making purchases within recent months?**  \n",
        "     This explores customer purchase frequency, allowing for the identification of repeat buyers and the overall engagement level of the customer base.\n",
        "\n",
        "   - **Which are the top 5 customers in terms of monetary value spent in recent months?**  \n",
        "     This question highlights the most valuable customers based on their total spending, aiding in loyalty programs or targeted marketing efforts.\n",
        "\n",
        "4. **Which states have the highest and lowest total sales, and what regional patterns can be identified?**  \n",
        "   This geographic analysis will reveal which regions are contributing the most to sales revenue and which areas may require more focus.\n",
        "\n",
        "5. **What is the relationship between product pricing, shipping costs, and customer review scores?**  \n",
        "   Through clustering analysis, this question seeks to understand how product price and shipping cost impact customer satisfaction, as reflected in review scores, to optimize pricing strategies and improve customer experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-z4QGlO8DC1"
      },
      "source": [
        "## Import All Packages/Libraries Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FVYwaObI8DC1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Sh51Xy8DC1"
      },
      "source": [
        "## Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXU2GBYu8DC1"
      },
      "source": [
        "### Gathering Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zjCBk1BI8DC1"
      },
      "outputs": [],
      "source": [
        "customers_df = pd.read_csv('data/customers_dataset.csv')\n",
        "geolocation_df = pd.read_csv('data/geolocation_dataset.csv')\n",
        "order_items_df = pd.read_csv('data/order_items_dataset.csv')\n",
        "order_payments_df = pd.read_csv('data/order_payments_dataset.csv')\n",
        "order_reviews_df = pd.read_csv('data/order_reviews_dataset.csv')\n",
        "orders_df = pd.read_csv('data/orders_dataset.csv')\n",
        "product_category_df =pd.read_csv('data/product_category_name_translation.csv')\n",
        "products_df = pd.read_csv('data/products_dataset.csv')\n",
        "sellers_df = pd.read_csv('data/sellers_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHSiqaZp8DC1"
      },
      "source": [
        "### Assessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `customer` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax-3tEjc9Cj1"
      },
      "outputs": [],
      "source": [
        "customers_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', customers_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `geolocation` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geolocation_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicated_geo = geolocation_df.duplicated().sum()\n",
        "print('Number of duplicates: ', duplicated_geo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geolocation_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `order_items` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_df = order_items_df.drop('shipping_limit_date', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', order_items_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `order_payments` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_payments_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', order_payments_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_payments_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `order_reviews` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_reviews_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_reviews_df = order_reviews_df.drop(['review_creation_date','review_answer_timestamp', 'review_comment_title', 'review_comment_message'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_reviews_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates:', order_reviews_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_reviews_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `orders` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.order_status.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df = orders_df.loc[orders_df['order_status'] != 'unavailable']\n",
        "orders_df = orders_df.loc[orders_df['order_status'] != 'canceled']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', orders_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `product_category_name_translation` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_category_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', product_category_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `product` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df = products_df[['product_id','product_category_name']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', products_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assess `sellers` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of duplicates: ', sellers_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN5R4hr8DC1"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Remove Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset that has duplicate data is only found in the `geolocation` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "geolocation_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Current Duplicate Count: ', geolocation_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Change Data Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains incorrect data types in the following columns:\n",
        "- `orders_df`: The columns `order_purchase_timestamp`, `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date`, and `order_estimated_delivery_date` are currently not in the correct format. These columns should be converted to the datetime data type for proper handling of time-based operations and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "date_time = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
        "\n",
        "for column in date_time:\n",
        "  orders_df[column] = pd.to_datetime(orders_df[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that the columns `order_purchase_timestamp`, `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date`, and `order_estimated_delivery_date` now have the correct data type, which is **datetime**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View Inaccuracy Data in **datetime**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The order of events is as follows:\n",
        "`order_purchase_timestamp` < `order_approved_at` < `order_delivered_carrier_date` < `order_delivered_customer_date`, and `order_delivered_carrier_date` < `order_estimated_delivery_date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wrong_order = orders_df[\n",
        "    (orders_df['order_purchase_timestamp'] > orders_df['order_approved_at']) |\n",
        "    (orders_df['order_approved_at'] > orders_df['order_delivered_carrier_date']) |\n",
        "    (orders_df['order_delivered_carrier_date'] > orders_df['order_delivered_customer_date'])|\n",
        "    (orders_df['order_delivered_carrier_date'] > orders_df['order_estimated_delivery_date'])\n",
        "]\n",
        "wrong_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are numerous incorrect data entries. As a solution, the incorrect values will be replaced with the previous datetime values from the respective order.\n",
        "\n",
        "For example, if there is an error in the `order_approved_at` column, the value will be replaced with the data from `order_purchase_timestamp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.loc[orders_df['order_purchase_timestamp'] > orders_df['order_approved_at'], 'order_approved_at'] = orders_df['order_purchase_timestamp']\n",
        "orders_df.loc[orders_df['order_approved_at'] > orders_df['order_delivered_carrier_date'], 'order_delivered_carrier_date'] = orders_df['order_approved_at']\n",
        "orders_df.loc[orders_df['order_delivered_carrier_date'] > orders_df['order_delivered_customer_date'], 'order_delivered_customer_date'] = orders_df['order_delivered_carrier_date']\n",
        "orders_df.loc[orders_df['order_delivered_carrier_date'] > orders_df['order_estimated_delivery_date'], 'order_estimated_delivery_date'] = orders_df['order_delivered_carrier_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wrong_order = orders_df[\n",
        "    (orders_df['order_purchase_timestamp'] > orders_df['order_approved_at']) |\n",
        "    (orders_df['order_approved_at'] > orders_df['order_delivered_carrier_date']) |\n",
        "    (orders_df['order_delivered_carrier_date'] > orders_df['order_delivered_customer_date'])|\n",
        "    (orders_df['order_delivered_carrier_date'] > orders_df['order_estimated_delivery_date'])\n",
        "]\n",
        "wrong_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The incorrect data has been resolved and is no longer present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Handling Missing Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Several datasets contain missing values:\n",
        "\n",
        "- `orders` dataset: Missing values are found in the columns `order_approved_at`, `order_delivered_carrier_date`, and `order_delivered_customer_date`.\n",
        "- `products` dataset: The column `product_category_name_id` has missing values.\n",
        "\n",
        "These missing values should be addressed to ensure data quality and accuracy in the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset `order`\n",
        "orders_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The missing values will be filled using the values from the preceding column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.loc[orders_df['order_approved_at'].isna(), 'order_approved_at'] = orders_df['order_purchase_timestamp']\n",
        "orders_df.loc[orders_df['order_delivered_carrier_date'].isna(), 'order_delivered_carrier_date'] = orders_df['order_approved_at']\n",
        "orders_df.loc[orders_df['order_delivered_customer_date'].isna(), 'order_delivered_customer_date'] = orders_df['order_delivered_carrier_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is now clean, with no missing values remaining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset `product`\n",
        "products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this dataset, the missing values cannot be filled (since category names must be accurate). Therefore, all rows containing missing values will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df = products_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is now clean, with no missing values remaining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fix Inaccuracy Value or Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Dataset `order_items`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Outlier detection will be performed on the `price` and `freight_value` columns.\n",
        "2. A merge between `products_df` and `product_category_df` will be done to retrieve the product names in English, resulting in `products_in_english_df`.\n",
        "3. A merge between `order_items_df` and `products_in_english_df` will be conducted to determine whether the data points are truly outliers, resulting in `order_items_product_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_df.nlargest(5, 'price')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that many data points have high prices. Let's now check for the lowest prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_df.nsmallest(5, 'price')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It cannot yet be confirmed whether the data are outliers, as there may be genuinely expensive products. A merge with the product information will be performed to verify this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_in_english_df = pd.merge(\n",
        "    left=products_df,\n",
        "    right=product_category_df,\n",
        "    how='left',\n",
        "    left_on='product_category_name',\n",
        "    right_on='product_category_name'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_items_product_df = pd.merge(\n",
        "    left=order_items_df,\n",
        "    right=products_in_english_df,\n",
        "    how='left',\n",
        "    left_on='product_id',\n",
        "    right_on='product_id'\n",
        ")\n",
        "descending_order_products = order_items_product_df.sort_values(by='price', ascending=False)\n",
        "descending_order_products.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ascending_order_products = order_items_product_df.sort_values(by='price', ascending=True)\n",
        "ascending_order_products.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that there are no outliers in the `order_items` data for both `price` and `freight_value`. The high `freight_value` may be due to long shipping distances. As for the `order_item_id` column, since there are many product items, there are naturally multiple `order_item_id` values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Dataset `order_payment`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. To check for outliers in the `payment_value`, `payment_sequential`, and `payment_installments` columns, a merge will be performed on `order_items_product_df`, resulting in `order_payments_items_products_df`.\n",
        "2. The outliers are mainly caused by duplicate `order_id values`, so those with the same `order_id` need to be consolidated, resulting in `result_order_payments_items_product_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_payments_df.nlargest(5, 'payment_value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_payments_items_products_df = pd.merge(\n",
        "    left=order_items_product_df,\n",
        "    right=order_payments_df,\n",
        "    how='left',\n",
        "    left_on='order_id',\n",
        "    right_on='order_id'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "descending_order_payments = order_payments_items_products_df.sort_values(by='payment_value', ascending=False)\n",
        "descending_order_payments.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jika dilihat dari order_id, kemungkinan dengan payment_value sebesar itu dikarenakan membeli dengan jumlah yang banyak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order_payments_items_products_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dapat dilihat bahwa adanya perbedaan antara banyaknya order_id dengan value yang unique pada order_id. Maka, kita harus:\n",
        "1. Mengganti order_item_id dengan order_item dan mengambil nilai maksimalnya.\n",
        "2. Menyatukan semua order_id yang sama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_order_payments_items_products_df = order_payments_items_products_df.loc[order_payments_items_products_df.groupby('order_id')['order_item_id'].idxmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_order_payments_items_products_df.sort_values(by='payment_value', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Kesimpulan:**\n",
        "Problem masalah terselesaikan. Payment_value yang tinggi disebabkan karena order_id yang menumpuk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Berdasarkan Payment sequential\n",
        "result_order_payments_items_products_df['payment_sequential'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ternyata terdapat missing value. Hal ini dapat disebabkan karena penggabungan dataset (merge). Caranya adalah, kita akan menghapus semua missing value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_order_payments_items_products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_order_payments_items_products_df.dropna(inplace=True)\n",
        "result_order_payments_items_products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sudah tidak terdapat missing value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_order_payments_items_products_df.sort_values(by='payment_sequential', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Kesimpulan:**\n",
        "\n",
        "Payment_sequential yang tinggi kemungkinan karena proses pembayaran atau urutan pembayarannya yang terbilang cukup banyak. Dan untuk payment_installment (pembayaran cicilan) tergantung dari kesepakatan awal pembelian. Jadi tidak mempengaruhi data outlier atau inaccuracy data jika < 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp-Y6wU38DC1"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploration of all the `orders` Dataset with the `products` Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merging The `orders` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_item_df = pd.merge(\n",
        "    left=orders_df,\n",
        "    right=order_items_df,\n",
        "    how='left',\n",
        "    left_on='order_id',\n",
        "    right_on='order_id'\n",
        ")\n",
        "orders_payment_df = pd.merge(\n",
        "    left=orders_item_df,\n",
        "    right=order_payments_df,\n",
        "    how='left',\n",
        "    left_on='order_id',\n",
        "    right_on='order_id'\n",
        ")\n",
        "\n",
        "all_orders_df = pd.merge(\n",
        "    left=orders_payment_df,\n",
        "    right=order_reviews_df,\n",
        "    how='left',\n",
        "    left_on='order_id',\n",
        "    right_on='order_id'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the merging of datasets, a significant number of missing values may arise. It is essential to handle and clean these missing values to ensure data quality and accuracy for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df = all_orders_df.loc[all_orders_df.groupby('order_id')['order_item_id'].idxmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, since we have already replaced the `order_item_id` with the largest `order_item_id`, rename it to `sum_order`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_orders_df.rename(columns={'order_item_id': 'qty_order'}, inplace=True)\n",
        "all_orders_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merging The `products` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df = pd.merge(\n",
        "    left=products_df,\n",
        "    right=product_category_df,\n",
        "    how='left',\n",
        "    left_on='product_category_name',\n",
        "    right_on='product_category_name'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merging The `orders` Dataset and The `products` Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df = pd.merge(\n",
        "    left=all_orders_df,\n",
        "    right=products_df,\n",
        "    how='left',\n",
        "    left_on='product_id',\n",
        "    right_on='product_id'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Missing values have occurred due to the merging of datasets. These missing values will be removed to ensure data integrity for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Identifying Which Products Have The Highest and Lowest Review Scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.groupby(by='product_category_name_english').agg({\n",
        "    'product_id':'nunique',\n",
        "    'qty_order':'sum',\n",
        "    'review_score':'mean'\n",
        "}).sort_values(by=['review_score','qty_order'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that:\n",
        "- The highest review score is for the cds_dvds_musicals category, with a score of 4.6. This category has a total of 46 orders, but only consists of 1 product type.\n",
        "- The lowest review score is for the security_and_services category, with a score of 5. This category has a total of 2 orders and includes 2 different product types.\n",
        "\n",
        "This highlights a significant difference in both review scores and order volumes between the two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Examining The Time Intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The time interval between when the order was placed and when it was approved.\n",
        "- The time interval from approval to handing over to the shipping service.\n",
        "- The shipping duration.\n",
        "- The time interval between the estimated delivery date and the actual delivery to the customer.\n",
        "- The time interval from the order placement date to the actual delivery to the customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.order_status.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# time interval between order placement and approval\n",
        "orders_products_copy = orders_products_df.copy()\n",
        "apply_time = orders_products_copy['order_approved_at'] - orders_products_copy['order_purchase_timestamp']\n",
        "apply_time = apply_time.apply(lambda x: x.total_seconds())\n",
        "orders_products_df['apply_time'] = round(apply_time/86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# time interval from approval to handover to the shipping service\n",
        "shipped_time = orders_products_copy.loc[orders_products_copy['order_status'] != 'created']\n",
        "shipped_time = shipped_time['order_delivered_carrier_date'] - shipped_time['order_approved_at']\n",
        "shipped_time = shipped_time.apply(lambda x: x.total_seconds())\n",
        "orders_products_df['shipped_time'] = round(shipped_time/86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shipping time interval\n",
        "customer_gets_order = orders_products_copy.loc[~orders_products_copy['order_status'].isin(['created', 'processing', 'approved', 'invoiced'])]\n",
        "customer_gets_order = customer_gets_order['order_delivered_customer_date'] - customer_gets_order['order_delivered_carrier_date']\n",
        "customer_gets_order = customer_gets_order.apply(lambda x: x.total_seconds())\n",
        "orders_products_df['customer_gets_order'] = round(customer_gets_order/86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# time interval between the estimated delivery and actual delivery to the customer\n",
        "estimated_range = orders_products_copy.loc[~orders_products_copy['order_status'].isin(['created', 'processing', 'approved', 'invoiced'])]\n",
        "estimed_range = estimated_range['order_estimated_delivery_date'] - estimated_range['order_delivered_carrier_date']\n",
        "estimed_range = estimed_range.apply(lambda x: x.total_seconds())\n",
        "orders_products_df['estimated_range'] = round(estimed_range/86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# time interval from the order date to the delivery to the customer\n",
        "range_order = orders_products_copy['order_delivered_customer_date'] - orders_products_copy['order_purchase_timestamp']\n",
        "range_order = range_order.apply(lambda x: x.total_seconds())\n",
        "orders_products_df['range_order'] = round(range_order/86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.agg({\n",
        "    'apply_time':['mean', 'min', 'max'],\n",
        "    'shipped_time':['mean', 'min', 'max'],\n",
        "    'customer_gets_order':['mean', 'min', 'max'],\n",
        "    'estimated_range':['mean','min','max'],\n",
        "    'range_order':['mean', 'min', 'max']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusion:**\n",
        "- The time interval between order placement and approval ranges from 0 to 60 days, with an average of less than 1 day (0.23 days).\n",
        "- The time interval from approval to handover to the shipping service ranges from 0 to 107 days, with an average of 2-3 days (7.883 days).\n",
        "- The shipping time interval ranges from 0 to 205 days, with an average of 9 days (9.14 days).\n",
        "- The time interval between the estimated delivery and when the order reaches the customer ranges from 0 to 148 days, with an average estimated delivery time of 20-21 days (20.42 days).\n",
        "- The time interval from the order date to the delivery to the customer ranges from 0 to 208 days (0 likely due to orders being canceled or input errors that have been corrected), with an average of 12 days (12.30 days)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analyzing The Rating Based on The Number of Orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.order_status.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_df.groupby(by='review_score').agg({\n",
        "    'qty_order': 'sum',\n",
        "    'product_category_name_english': 'unique',\n",
        "    'payment_value': ['min', 'max']\n",
        "}).sort_values(('qty_order', 'sum'), ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that there are 62,284 orders with a rating of 5, 20,887 orders with a rating of 4, and 13,438 orders with a rating of 1. This indicates a need for evaluation and improvement in products under the product_category_name_english that received a rating of 1. Addressing these issues is crucial to prevent further customer dissatisfaction and to avoid lowering the overall rating of these products in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging The `customers` Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Renaming the columns customer_zip_code_prefix and geolocation_zip_code_prefix to zip_code_prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "geolocation_df.rename(columns={'geolocation_zip_code_prefix': 'zip_code_prefix'}, inplace=True)\n",
        "customers_df.rename(columns={'customer_zip_code_prefix': 'zip_code_prefix'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df = pd.merge(\n",
        "    left=customers_df,\n",
        "    right=geolocation_df,\n",
        "    how='left',\n",
        "    left_on='zip_code_prefix',\n",
        "    right_on='zip_code_prefix'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that the customer_id and customer_unique_id should have the same values for both count and unique entries. This means there are duplicate customer_id entries. We will retain only one customer_id for each unique value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_duplicates = customers_df.drop_duplicates('customer_id', keep='first')\n",
        "customers_df = pd.concat([ \n",
        "    drop_duplicates, customers_df[~customers_df.duplicated('customer_id', keep=False)]\n",
        "])\n",
        "customers_df = customers_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df = customers_df.drop_duplicates(subset='customer_id', keep='first').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging The `sellers` Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Renaming the seller_zip_code_prefix column, then merging it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.rename(columns={'seller_zip_code_prefix': 'zip_code_prefix'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df = pd.merge(\n",
        "    left=sellers_df,\n",
        "    right=geolocation_df,\n",
        "    how='left',\n",
        "    left_on='zip_code_prefix',\n",
        "    right_on='zip_code_prefix'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same issue occurs in the sellers dataset, where there are non-unique seller IDs. Let's check it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same seller IDs will be merged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_duplicates = sellers_df.drop_duplicates('seller_id', keep='first')\n",
        "sellers_df = pd.concat([ \n",
        "    drop_duplicates, sellers_df[~sellers_df.duplicated('seller_id', keep=False)]\n",
        "])\n",
        "sellers_df = sellers_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df = sellers_df.drop_duplicates(subset='seller_id', keep='first').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sellers_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data has been corrected. Next, the task is to merge all the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore All Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merging All Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_products_customers_df = pd.merge(\n",
        "    left=orders_products_df,\n",
        "    right=customers_df,\n",
        "    how='left',\n",
        "    left_on='customer_id',\n",
        "    right_on='customer_id'\n",
        ")\n",
        "\n",
        "all_df = pd.merge(\n",
        "    left=orders_products_customers_df,\n",
        "    right=sellers_df,\n",
        "    how='left',\n",
        "    left_on='seller_id',\n",
        "    right_on='seller_id'\n",
        ")\n",
        "\n",
        "all_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View Monthly Sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df.loc[:, 'month_year'] = all_df['order_purchase_timestamp'].dt.strftime('%Y-%m')\n",
        "monthly_salaries = all_df.groupby('month_year')['payment_value'].sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monthly_salaries.sort_values(by='payment_value', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be observed that the highest revenue occurred in April 2018, amounting to $1,115,553.30, while the lowest revenue was in December 2016, totaling $19.62."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Top 5 Best Selling and Least Selling Product Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_category = all_df.groupby(by='product_category_name_english').agg({\n",
        "    'product_id': 'nunique',\n",
        "    'qty_order': 'sum',\n",
        "    'payment_value': ['min', 'max']\n",
        "})\n",
        "product_category.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "most_purchases = product_category.sort_values(by=('qty_order', 'sum'), ascending=False)\n",
        "most_purchases.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that the top 5 best-selling product categories are bed_bath_table, health_beauty, sports_leisure, furniture_decor, and computers_accessories. These categories dominate in terms of total orders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "least_purchases = product_category.sort_values(by=('qty_order', 'sum'), ascending=True)\n",
        "least_purchases.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the other hand, the categories with the fewest sales are security_and_services, fashion_childrens_clothes, la_cuisine, cds_dvds_musicals, and arts_and_craftmanship. These categories have significantly lower order volumes compared to the top sellers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyZjqak8DC2"
      },
      "source": [
        "## Visualization & Explanatory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZxOiQ6n8DC2"
      },
      "source": [
        "### Question 1: Which product categories have the highest and lowest sales volumes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_visualization = all_df.groupby('product_category_name_english')['qty_order'].sum().reset_index()\n",
        "bottom = products_visualization.sort_values(by='qty_order', ascending=True).head(5)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16,8))\n",
        "colors = [\"#72BCD4\", \"#D3D3D3\", \"#D3D3D3\", \"#D3D3D3\", \"#D3D3D3\"]\n",
        "\n",
        "# Top 5 product categories by quantity ordered\n",
        "sns.barplot(x='qty_order', y='product_category_name_english', \n",
        "            data=products_visualization.sort_values(by='qty_order', ascending=False).head(5), \n",
        "            hue='product_category_name_english', palette=colors, ax=ax[0], dodge=False, width=0.5)\n",
        "\n",
        "ax[0].set_xlabel('Total Quantity Ordered', fontsize=14, fontweight='bold')\n",
        "ax[0].set_ylabel('Product Category Name', fontsize=14, fontweight='bold')\n",
        "ax[0].set_title('Top 5 Best Selling Product Categories', loc='center', fontsize=16, fontweight='bold')\n",
        "ax[0].tick_params(axis='y', labelsize=12)\n",
        "ax[0].tick_params(axis='x', labelsize=12)\n",
        "ax[0].legend(title='Product Category', loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12, title_fontsize=14)\n",
        "\n",
        "# Bottom 5 product categories by quantity ordered\n",
        "sns.barplot(x='qty_order', y='product_category_name_english', \n",
        "            data=bottom.head(5), \n",
        "            hue='product_category_name_english', palette=colors, ax=ax[1], dodge=False, width=0.5)\n",
        "\n",
        "ax[1].set_xlabel('Total Quantity Ordered', fontsize=14, fontweight='bold')\n",
        "ax[1].set_ylabel('Product Category Name', fontsize=14, fontweight='bold')\n",
        "ax[1].set_title('Top 5 Worst Selling Product Categories', loc='center', fontsize=16, fontweight='bold')\n",
        "ax[1].tick_params(axis='y', labelsize=12)\n",
        "ax[1].tick_params(axis='x', labelsize=12)\n",
        "ax[1].legend(title='Product Category', loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12, title_fontsize=14)\n",
        "\n",
        "plt.suptitle('Top 5 Best and Worst Selling Product Categories by Quantity Ordered', fontsize=20, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 0.95])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Top 5 Best Selling Product Categories:**\n",
        "- The bed_bath_table category leads as the top-selling product, with a significantly higher quantity sold compared to other categories.\n",
        "- The other top categorieshealth_beauty, sports_leisure, furniture_decor, and computers_accessoriesalso demonstrate strong sales, but they lag behind bed_bath_table in total quantity ordered.\n",
        "\n",
        "**Top 5 Worst Selling Product Categories:**\n",
        "- The security_and_services category has the lowest quantity of products sold, followed by fashion_childrens_clothes, la_cuisine, cds_dvds_musicals, and arts_and_craftmanship, which also show very low sales volumes.\n",
        "- The gap between the top and bottom categories is vast, showing that certain product categories perform much better in terms of sales compared to others, with bottom categories barely registering significant sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHI7CiU8DC2"
      },
      "source": [
        "### Question 2: What are the monthly sales trends, and how have they evolved over time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monthly_sales = all_df.groupby('month_year')['payment_value'].sum().reset_index()\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=monthly_sales, x=\"month_year\", y=\"payment_value\", marker=\"o\", linestyle=\"-\", color='#4C72B0')\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.ylabel(\"Total Payment Value ($)\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Month-Year\", fontsize=14, fontweight='bold')\n",
        "plt.title(\"Monthly Sales Volume Over Time\", fontsize=16, fontweight='bold')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the chart, the following conclusions can be drawn:\n",
        "1. The timeline in the chart starts from September 2016 and ends in September 2018.\n",
        "2. There are noticeable increases in sales in the months of October 2016, January 2017, February 2017, March 2017, May 2017, July 2017, August 2017, September 2017, October 2017, November 2017, January 2018, March 2018, April 2018, and July 2018.\n",
        "3. There are decreases in sales during December 2016, April 2017, June 2017, December 2017, February 2018, May 2018, June 2018, August 2018, and September 2018.\n",
        "4. A significant spike occurred in November 2017, along with a consistent upward trend from January 2017 to March 2017.\n",
        "5. A drastic drop in sales is seen in September 2018, which could be due to incomplete data processing or ongoing record updates for that month.\n",
        "\n",
        "This indicates general growth in sales over time, with fluctuations possibly related to seasonality or data completion issues for September 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: The RFM (Recency, Frequency, Monetary) analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = all_df['order_purchase_timestamp'].max()\n",
        "rfm_df = all_df.groupby(by='customer_id', as_index=False).agg({\n",
        "    'order_purchase_timestamp':lambda x: (now - x.max()).days,\n",
        "    'order_id': 'count',\n",
        "    'payment_value':'sum'\n",
        "})\n",
        "rfm_df.columns = ['customer_id', 'recency', 'frequency', 'monetary']\n",
        "rfm_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the customer IDs are quite long, we will replace them with numeric IDs. This change does not have any impact, as we are only interested in seeing when the last transaction occurred, how frequently the customer makes purchases, and the total revenue generated by each customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rfm_df['numeric_id'] = pd.factorize(rfm_df['customer_id'])[0] + 1\n",
        "rfm_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30,6))\n",
        "colors = sns.color_palette(\"husl\", 5)\n",
        "\n",
        "# Bar plot for Recency (Last Purchase)\n",
        "sns.barplot(y=\"recency\", x='numeric_id', data=rfm_df.sort_values(by='recency', ascending=True).head(5), hue='numeric_id', palette=colors, ax=ax[0], dodge=False, width=0.5)\n",
        "ax[0].set_ylabel(None)\n",
        "ax[0].set_xlabel(None)\n",
        "ax[0].set_title('Last Purchase (days)', loc='center', fontsize=18, fontweight='bold')\n",
        "ax[0].tick_params(axis='x', labelsize=15)\n",
        "ax[0].tick_params(axis='y', labelsize=12)\n",
        "ax[0].legend(title='Customer ID', fontsize=12, title_fontsize=14, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "# Bar plot for Frequency (Number of Purchases)\n",
        "sns.barplot(y='frequency', x='numeric_id', data=rfm_df.sort_values(by='frequency', ascending=False).head(5), hue='numeric_id', palette=colors, ax=ax[1], dodge=False, width=0.5)\n",
        "ax[1].set_ylabel(None)\n",
        "ax[1].set_xlabel(None)\n",
        "ax[1].set_title('Purchase Frequency', loc='center', fontsize=18, fontweight='bold')\n",
        "ax[1].tick_params(axis='x', labelsize=15)\n",
        "ax[1].tick_params(axis='y', labelsize=12)\n",
        "ax[1].legend(title='Customer ID', fontsize=12, title_fontsize=14, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "# Bar plot for Monetary (Total Money Spent)\n",
        "sns.barplot(y='monetary', x='numeric_id', data=rfm_df.sort_values(by='monetary', ascending=False).head(5), hue='numeric_id', palette=colors, ax=ax[2], dodge=False, width=0.5)\n",
        "ax[2].set_ylabel(None)\n",
        "ax[2].set_xlabel(None)\n",
        "ax[2].set_title('Total Amount Spent', loc='center', fontsize=18, fontweight='bold')\n",
        "ax[2].tick_params(axis='x', labelsize=15)\n",
        "ax[2].tick_params(axis='y', labelsize=12)\n",
        "ax[2].legend(title='Customer ID', fontsize=12, title_fontsize=14, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "plt.suptitle('Best Customers Based on RFM Parameters', fontsize=22, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the data visualization above, the following conclusions can be drawn:\n",
        "1. **Recency (Last Purchase)**: Some customers have made transactions within the last 0 to 4 days, with the most recent transaction made by the customer with ID `28005`.\n",
        "2. **Frequency (Purchase Frequency)**: Most customers have made purchases once in the last few months, indicating relatively infrequent purchase patterns.\n",
        "3. **Monetary (Total Amount Spent)**: The amount of money spent by customers varies. The customer with ID `8201` has spent the most, showing significant financial involvement compared to other customers.\n",
        "\n",
        "From the three RFM parameters, the top customers who are the most active and spend the most can be identified within the analyzed period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Which states have the highest and lowest total sales, and what regional patterns can be identified?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sales_by_state = all_df.groupby('customer_state')['payment_value'].sum().sort_values(ascending=True)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sales_by_state.plot(kind='barh', color='skyblue', edgecolor='black')\n",
        "plt.title('Total Sales by State', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('State', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Total Sales ($)', fontsize=14, fontweight='bold')\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the total sales by state chart, it can be concluded that the state with the highest sales is So Paulo (SP), with a significant gap compared to other states. On the other hand, the states with the lowest sales are Roraima (RR), Amap (AP), Acre (AC), and Amazonas (AM). This indicates an uneven distribution of sales, where larger or more developed states tend to have much higher sales volumes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: What is the relationship between product pricing, shipping costs, and customer review scores?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.figure(figsize=(12,8))\n",
        "scatter = plt.scatter(all_df['price'], all_df['freight_value'], \n",
        "                      c=all_df['review_score'], cmap='viridis', s=80, alpha=0.7, edgecolor='w', linewidth=0.5)\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Review Score', fontsize=12)\n",
        "plt.xlabel('Product Price ($)', fontsize=14)\n",
        "plt.ylabel('Shipping Cost ($)', fontsize=14)\n",
        "plt.title('Relationship between Product Price, Shipping Cost, and Review Score', fontsize=16, fontweight='bold')\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most products are in the lower range of price and shipping cost. However, higher-priced products tend to receive better review scores, even though their shipping costs are also higher. This suggests that other factors, such as quality, influence customers' purchasing decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WeHlCeX8DC2"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTcyR48Y8DC2"
      },
      "source": [
        "1. **Which product categories have the highest and lowest sales volumes?**  \n",
        "   The top-selling product categories are **bed_bath_table, health_beauty, sports_leisure, furniture_decor,** and **computers_accessories**. Conversely, the categories with the least sales are **security_and_services, fashion_childrens_clothes, la_cuisine, cds_dvds_musicals,** and **art_and_craftmanship**. This indicates that home-related and technology products are the most popular, while niche categories like arts and children's fashion have lower demand.\n",
        "\n",
        "2. **What are the monthly sales trends, and how have they evolved over time?**  \n",
        "   The analysis of monthly sales trends reveals key patterns:\n",
        "   - The data spans from September 2016 to September 2018.\n",
        "   - Significant sales increases occurred in months like October 2016, January to March 2017, May, July, and September 2017, as well as November 2017 and early 2018.\n",
        "   - Declines were observed in December 2016, April, June, and December 2017, and sporadically in 2018.\n",
        "   - November 2017 showed a substantial spike in sales, possibly driven by promotional events, followed by a consistent growth period in early 2017.\n",
        "   - The sharp decline in September 2018 may be attributed to incomplete data collection at the time of analysis.\n",
        "\n",
        "3. **RFM Analysis**:\n",
        "   - **Recency**: Most customers made purchases within the last 0-4 days of the dataset, with the most recent transaction by customer ID 28005.\n",
        "   - **Frequency**: The majority of customers have made only **one purchase** in recent months, indicating a large proportion of single-purchase behavior.\n",
        "   - **Monetary**: Spending varies widely, but customer ID 8201 is the highest spender, showing a significant contribution to overall revenue.\n",
        "\n",
        "4. **Which states have the highest and lowest total sales, and what regional patterns can be identified?**  \n",
        "   From the geo-analysis, **Sao Paulo** stands out as the state with the highest sales by a significant margin, indicating a strong customer base in this region. On the other hand, **Roraima (RR), Amapa (AP), Acre (AC),** and **Amazonas (AM)** are the states with the lowest sales, suggesting potential opportunities for market expansion or targeted campaigns in these regions.\n",
        "\n",
        "5. **What is the relationship between product pricing, shipping costs, and customer review scores?**  \n",
        "   The clustering analysis shows that most products are within the lower price and shipping cost range. However, higher-priced products tend to receive better review scores, even though their shipping costs are also relatively high. This suggests that customers are willing to pay a premium for higher quality products, indicating that price alone is not a significant barrier when the perceived value and customer satisfaction are high."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "972b3bf27e332e87b5379f2791f6ef9dfc79c71018c370b0d7423235e20fe4d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
